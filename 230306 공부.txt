exact matching


엘라스틱 서치
	- 일반적으로 RDB는 행을 기반으로 데이터 저장. 이에 반해 엘라스틱서치는 단어를 기반으로 저장.
	- RDB는 데이터 수정 삭제의 편의, 속도 면에 강점이 있지만 다양한 조건의 데이터를 검색 및 집계하는 데에 구조적 한계가 있음. 특정 단어를 검색하는 경우 document의 개수만큼 해당 단어가 어디에 있는지 확인해야 한다. 즉 document의 수가 많으면 비효율적임.
	- 단어 기반 데이터를 저장하는 엘라스틱 서치는 특정 단어가 어디에 저장되어 있는지 알고 있어 도큐먼트 검색이 필요가 없다. 
	- 수정 삭제는 리소스가 많이 소요된다.
	- 엘라스틱서치는 NoSQL 기반의 문서 지향 DB


pre-training 
	- feature-based:
		어떠한 특정 task를 해결하기 위한 아키텍쳐를 구성하며 pre-trained representations를 부가적인 피쳐로 사용하는 것	
	- fine-tuning:
		BERT에서 중점적으로 사용하는 접근방법.
		특정 task에 대한 파라미터를 최소화하여 범용적인 pre-trained 모델을 사용.	
		

ELMo(Embeddings from Language Model) 언어 모델로 하는 임베딩
	pre-trained 언어 모델을 사용함.
	https://wikidocs.net/33930


MLM(Masked Language Model)

BERT(Bidirectional Encoder Representations from Transfomers)
	트랜스포머의 인코더를 사용해 양방향 학습 모델??


ELECTRA 모델
	- 인풋을 마스킹하지 않고 generateor network(생성 네트워크)를 통해 토큰을 다른 대안으로 대체해 인풋을 손상
	- 토큰의 ID를 예측하는 모델 대신 discriminator network(판별 네트워크)를 통해 각 토큰이 생성된 샘플로 대체되었는지 여부 예측.
	- 그러니까 쉬운 말로 인풋의 토큰 일부를 다른 토큰으로 대체한 후 판별자가 바뀐건지 아닌지 판단하며 학습하는 것 같다.
